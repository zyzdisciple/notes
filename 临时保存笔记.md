# SparkStreaming(1) ~ SparkStreaming编程指南

<font face="楷体">

之所以写这部分内容的原因是, 无论是网络上可以直接找到的资料, 还是出版的书籍种种, 版本大都在1.6~2.0不等, 且资源零零散散, 需要到处百度, 搜罗资源.

但根据个人开发了一段时间的感觉来看, 会遇到的绝大多数问题, 都可以在官方文档中找到答案.

因此也可以理解为这是官方文档的部分翻译.

个人英文水平有限, 如有错漏欢迎指正.

就目前来看, 主要分为这样几个板块.

1. [Spark Streaming Programming Guide](http://spark.apache.org/docs/latest/streaming-programming-guide.html) 也即SparkStreaming编程指南.

2. [Submitting Applications](http://spark.apache.org/docs/latest/submitting-applications.html) Spark部署发布相关

3. [Tuning Spark](http://spark.apache.org/docs/latest/tuning.html) Spark调优

4. [Spark Configuration](http://spark.apache.org/docs/latest/configuration.html) Spark可用配置, 可选参数.

目前已经有了Spark Streaming的中文翻译. 参考:

> [Spark Streaming编程指南](http://spark.apachecn.org/#/docs/6)

## Spark编程指南

内容本身会比较多, 因此会拆开来, 分多篇介绍.

在这里就不从word count的简单示例开始了, 而是直接从基础概念开始.

### 基础知识

#### Maven依赖

    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.11</artifactId>
        <version>2.4.3</version>
    </dependency>

    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming_2.12</artifactId>
        <version>2.4.3</version>
        <scope>provided</scope>
    </dependency>

而同样当前版本对应的中间件:

    
    Source	Artifact
    Kafka	spark-streaming-kafka-0-10_2.12
    Flume	spark-streaming-flume_2.12
    Kinesis spark-streaming-kinesis-asl_2.12 [Amazon Software License]

而更完整的, 更新的中间件 Maven 仓库路径为:

[Maven repository](https://search.maven.org/search?q=g:org.apache.spark%20AND%20v:2.4.3)

如果觉得欠缺什么, 不妨找找试试.

#### 初始化Streaming Context

为了初始化一个 Spark Streaming 程序,一个 StreamingContext 对象必须要被创建出来,它是所有的 Spark Streaming 功能的主入口点.

有两种创建方式:

    import org.apache.spark.*;
    import org.apache.spark.streaming.api.java.*;

    SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
    JavaStreamingContext ssc = new JavaStreamingContext(conf, new Duration(1000));

其中:

appName 是在Spark UI上展示所使用的名称.

master 是一个 [Spark, Mesos or YARN cluster URL](http://spark.apache.org/docs/latest/submitting-applications.html#master-urls), 不了解也没关系, 这部分会在Spark-submit时介绍到.

master 这个指的是Spark项目运行所在的集群. 如果想在本地启动SparkStreaming项目: 可以使用一个特殊的 “local[*]” , 启动Spark的本地模式, *表示会自动检测系统的内核数量. 

然而在集群环境下, 一般不采用硬编码的方式使用spark, 即 setMaster. 我们有更好的方式 通过 spark-submit 在提交时指定master参数即可.

需要注意到的是, 这句代码会在内部创建一个 SparkContext, 可以通过 ssc.sparkContext 访问使用.

batch interval 也即 new Duration(1000) 在这里指的是毫秒值, 还可以采用Durations来创建.

    Durations.seconds(5)

这个时间, 必须根据您的应用程序和可用的集群资源的等待时间要求进行设置.

另一种创建 SparkStreamingContext的方式为:

    import org.apache.spark.streaming.api.java.*;

    JavaSparkContext sc = ...   //existing JavaSparkContext
    JavaStreamingContext ssc = new JavaStreamingContext(sc, Durations.seconds(1));

在已经有了Context之后, 我们需要做的是:

1. 创建Input DStreams, 如kafka就有相应的方法可以创建DStream

2. 对输入流做 转换 处理, 也即我们的功能部分.

3. 开始接收输入并且使用 streamingContext.start() 来处理数据.

4. 使用 streamingContext.awaitTermination() 等待处理被终止(手动或者由于任何错误).

5. 使用 streamingContext.stop() 来手动的停止处理.

同时, 有几点需要注意的地方:

* 一旦一个 context 已经启动,将不会有新的数据流的计算可以被创建或者添加到它.

* 一旦一个 context 已经停止,它不会被重新启动.

* 同一时间内在 JVM 中只有一个 StreamingContext 可以被激活. 也即假设在使用SparkStreaming的同时, 需要依赖 SparkContext 或 SparkSQL等做一些操作, 此时不能重新创建 SparkContext 或是 SparkSQL(因为SparkSQL依然会创建Context.) 需要直接使用ssc.sparkContext. 

* 调用 ssc.stop() 会在停止 SparkStreamingContext的同时 停止 SparkContext. 如果需要仅停止 StreamingContext,需要使用 ssc.stop(false);

* 一个 SparkContext 就可以被重用以创建多个 StreamingContexts,只要前一个 StreamingContext 在下一个StreamingContext 被创建之前停止(不停止 SparkContext, 即使用 ssc.stop(false)).

在这里额外添加一条说明, ssc.stop 可以接收第二个参数, 是指定是否执行完当前批次的剩余数据.

#### Discretized Streams

Discretized Stream or DStream 是 Spark Streaming 提供的基本抽象.

有且仅有两种方式创建一个DStream, 第一种是通过 Spark的API去创建流, 第二种是从一个流转换成另一个流.

在内部,一个 DStream 被表示为一系列连续的 RDDs,它是 Spark 中一个不可改变的抽象,distributed dataset.在一个 DStream 中的每个 RDD 包含来自一定的时间间隔的数据,如下图所示.

![](http://spark.apache.org/docs/latest/img/streaming-dstream.png)

应用于 DStream 的任何操作转化为对于底层的 RDDs 的操作.例如,在 先前的示例,转换一个行(lines)流成为单词(words)中,flatMap 操作被应用于在行离散流(lines DStream)中的每个 RDD 来生成单词离散流(words DStream)的 RDDs.如下所示.

![](http://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)

因此对于RDD支持的操作, DStream也基本都支持.

#### Input DStreams 和 Receivers(接收器)

输入 DStreams 是代表输入数据是从流的源数据(streaming sources)接收到的流的 DStream.每一个 input DStream(除了 file stream 之外)与一个 Receiver 对象关联,它从 source(数据源)中获取数据,并且存储它到 Spark 的内存中用于处理.

receiver的java代码如下:

    class MyReceiver extends Receiver<String> {

        public MyReceiver(StorageLevel storageLevel) {
            //StorageLevel表示存储级别
            super(storageLevel);
        }

        public void onStart() {
            //1. 启动线程, 打开Socket连接, 准备开始接收数据
            //2. 启动一个非阻塞线程去接收数据.
            //3. 调用Store方法将数据存储到 Spark的内存中, store方法有多种实现,支持将多种多样的数据进行存储.
            //4. 在发生错误或异常时根据自身的处理策略调用stop, restart, reportError 方法.
        }

        public void onStop() {
            //清理各种线程,未关闭的链接等等
        }
    }

Spark Streaming 提供了两种内置的 streaming source(流的数据源).

* Basic sources(基础的数据源):在 StreamingContext API 中直接可以使用的数据源.例如:file systems 和 socket connections.

* Advanced sources(高级的数据源):像 Kafka,Flume,Kinesis,等等这样的数据源.可以通过对应的maven repository 找到依赖.

//待完善1

需要注意到的是, 如果你想要在你的流处理程序中并行的接收多个数据流,你可以创建多个 input DStreams.这将创建同时接收多个数据流的多个 receivers(接收器),然而,一个 Spark 的 worker/executor 是一个长期运行的任务(task),因此它将占用分配给 Spark Streaming 的应用程序的所有核中的一个核(core).

因此,需要记住,一个 Spark Streaming 应用需要分配足够的核(core)(或线程(threads),如果本地运行的话)来处理所接收的数据,以及来运行接收器(receiver(s)).

因此相应的就需要在创建master的时候 不要使用local[1] 或 local 仅分配一个线程, 这将会使得receiver得到一个线程 而对应的程序则没有线程可以处理.

在集群模式下, 则需要分配适当的核心数.

#### Basic Sources

* ssc.socketTextStream() 通过Socket来读取数据.

* 通过文件读取数据,  需要注意的是,文件系统必须是与 HDFS API 兼容的文件系统中(即,HDFS,S3,NFS 等),一个 DStream 可以像下面这样创建:

   声明: 对于下面所描述的这种方式,我个人并没有经过验证, 由于个人使用的数据源主要是kafka, mysql, es. 在尝试过程中fileStream并不能直接使用, 因此有以下猜想.

        streamingContext.textFileStream(dataDirectory);
    
   而windows的文件系统是 NTFS,这就要求我们有对应的文件环境才行.

   >参考:[Hadoop入门系列(一)Window环境下搭建hadoop和hdfs的基本操作](https://blog.csdn.net/qq_32938169/article/details/80209083)

   Spark Streaming 将监控dataDirectory 目录 以及 该目录中任何新建的文件 (写在嵌套目录中的文件是不支持的)

   需要注意的几个地方有:

   * 可以直接监控路径, 也可以通过目录的通配符来监控,hdfs://namenode:8040/logs/ 或 hdfs://namenode:8040/logs/2017/* 都是可以的.

   * 文件必须具有相同的数据格式.

   * 在读取文件的时候,参考的时间是 文件的更新时间,而非创建时间.

   * 一旦被加载,即使文件被更新了 在当前窗口内 也不会被重新读取, 因此即使文件不停被追加新的内容, 但是并不会读入.

   * 文件越多,检索文件变更情况所需要的时间也就越多,即使大多数文件都没有变更.

   * 如果使用的是通配符的方式 去识别文件目录,如: hdfs://namenode:8040/logs/2016-*, 在这种情况下, 通过重命名文件夹 也可以将对应文件夹下的文件 加入被监控的文件列表中, 当然需要修改时间在对应的window内.

   * 通过调用 FileSystem.setTimes() (hadoop api) 可以更改文件的更新时间.

   以下部分是对文件流的一个说明, 如何将文件转换成DStream的概念？

   >参考: org.apache.spark.streaming.dstream.FileInputDStream scala api

       *                      |<----- remember window ----->|
       * ignore threshold --->|                             |<--- current batch time
       *                      |____.____.____.____.____.____|
       *                      |    |    |    |    |    |    |
       * ---------------------|----|----|----|----|----|----|-----------------------> Time
       *                      |____|____|____|____|____|____|
       *                             remembered batches

   依然是按照时间批次来将数据转换成RDDs,整合成 DStream.

   在每个时间批次中,检测 被监控的文件列表 如果修改时间在 current batch范围内的, 在纳入列表, 转换成DStream, 在excutor的执行期间 新加入的文件,放入下一批次进行处理.

   而文件的修改时间 在 ignore threshold 之后的,则会被忽略掉.

   要求:

   * 运行Spark的系统时间 要与对应的 文件系统时间保持一致.

   * 文件必须在相同的文件系统下通过 atomically(原子的)moving(移动) 或 renaming(重命名) 到数据目录.

   而duration的定义则是通过:

   spark.streaming.fileStream.minRememberDuration

   默认是一分钟, 即读取修改时间在一分钟以内的文件.

   更细节的可以自行解读代码实现.

* Queue of RDDs as a Stream(RDDs 队列作为一个流)
  
  为了使用测试数据测试 Spark Streaming 应用程序,还可以使用 streamingContext.queueStream(queueOfRDDs) 创建一个基于 RDDs 队列的 DStream,每个进入队列的 RDD 都将被视为 DStream 中的一个批次数据,并且就像一个流进行处理.

#### Advanced Sources(高级数据源)

//待完善2

Kafka: Spark Streaming 2.4.3 要求 Kafka versions 0.8.2.1 以上版本.

官方参考链接: [Kafka Integration Guide](http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html)

个人参考链接: []

#### Custom Sources(自定义数据源)

DStreams 可以使用通过自定义的 receiver(接收器)接收到的数据来创建.

[Spark Streaming Custom Receivers](http://spark.apache.org/docs/latest/streaming-custom-receivers.html)

receiver的大致创建过程在上面已经提到过了.

案例代码实现:

[JavaCustomReceiver.java](https://github.com/apache/spark/blob/v2.4.3/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java).

通过如下方式使用 自定义的 Receiver
    
    // Assuming ssc is the JavaStreamingContext
    JavaDStream<String> customReceiverStream = ssc.receiverStream(new JavaCustomReceiver(host, port));
    JavaDStream<String> words = customReceiverStream.flatMap(s -> ...);

#### Receiver Reliability(接收器的可靠性)

有两种receiver, 可靠性, 不可靠性, 区别就在于对于数据的失败处理上, 可靠receiver并不会丢失数据,而不可靠receiver则不对 数据安全性 提供任何保障.

从数据源上来说, 本身就存在两种数据源, 如 kafka flume所提供的可靠性数据源.能够对 下发处理数据的 响应  做出相应的处理. 而不可靠数据源, 只负责下发数据.

//待完善3

如果想要实现一个 可靠的 receiver, 需要注意的是, 即使采用的是可靠数据源, 也不一定就是可靠的receiver.

如果你想实现一个可靠的数据接收器,必须用store方法,这是一个阻塞的方法,在它存入spark内部时才会返回值.如果接受器用的Storage Level 是复制(也可以使用默认),那么在复制完后才会得到返回值.因此,在确定完数据可靠存储之后,再适当的发送确认信息.这就避免了如果在存储中途失败,也不会导致数据丢失.因为缓冲区的数据没有被确认,那么数据源将会重新发送.

如果是不可靠接收器,那么无须以上逻辑,他只是简单地接收数据并存储到Spark内存中而已,但并非说不可靠的 接收器就毫无优点:

   * 系统会自动把数据分割为 大小合适的 块
   * 如果限制速率已经被指定, 那么系统会自动控制接收速率
   * 由于上面提到的优点, 因此实现起来更为简单.

而与之相对的, 可靠的接收器就需要自己实现数据分块, 以及速率控制, 而实现方式主要取决于数据源.

#### DStreams 上的 Transformations(转换)

DStreams 支持很多在 RDD 中可用的 transformation 算子, 至于transformation 和 action 算子的区别, 可以自行百度了解.

> 参考链接: [spark算子](https://www.jianshu.com/p/9556f04b9fd9)

而更详细的需要查看官方API

> 参考链接: [http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html](http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html)

其中返回值为各种RDD的一般都是 transformation算子, 否则为 action算子.

在茫茫多的 transformation中 选择几个比较特别的来详细说明下:

#### UpdateStateByKey

updateStateByKey 操作允许你维护任意状态,同时不断更新新信息.你需要通过两步来使用它:

1.  定义 state - state 可以是任何的数据类型.
2.  定义 state update function(状态更新函数)- 使用函数指定如何使用先前状态来更新状态,并从输入流中指定新值.

在每个 batch 中,Spark 会使用状态更新函数为所有已有的 key 更新状态,不管在 batch 中是否含有新的数据.如果这个更新函数返回一个 none,这个 key-value pair 也会被消除.

    Function2<List<Integer>, Optional<Integer>, Optional<Integer>> updateFunction =
    (values, state) -> {
        Integer newSum = ...  // add the new values with the previous running count to get the new count
        return Optional.of(newSum);
    };

    JavaPairDStream<String, Integer> runningCounts = pairs.updateStateByKey(updateFunction);

请注意,使用 updateStateByKey 需要配置的 checkpoint(检查点)的目录.

<font color="orange">

但是, updateStateByKey 有不可避免的缺点.

>参考: [[spark streaming] 状态管理 updateStateByKey&mapWithState](https://www.jianshu.com/p/3e4b9c5c6e02)

总结来说:

updateStateByKey底层是将preSateRDD和parentRDD进行co-group,然后对所有数据都将经过自定义的mapFun函数进行一次计算,即使当前batch只有一条数据也会进行这么复杂的计算,大大的降低了性能,并且计算时间会随着维护的状态的增加而增加.

mapWithstate底层是创建了一个MapWithStateRDD,存的数据是MapWithStateRDDRecord对象,一个Partition对应一个MapWithStateRDDRecord对象,该对象记录了对应Partition所有的状态,每次只会对当前batch有的数据进行跟新,而不会像updateStateByKey一样对所有数据计算.
</font>

因此才会有 mapWithstate 的性能远远高于 updateStateByKey

#### Transform 算子

这是Spark中 自由度最高的一个算子, Spark官方API提供的算子毕竟是有限的, 可能确实不能够满足你的要求, 因此才会有了这个 transform算子.

其核心作用是:

允许在 DStream 运行任何 RDD-to-RDD 函数.它能够被用来应用任何没在 DStream API 中提供的 RDD 操作.例如,连接数据流中的每个批(batch)和另外一个数据集的功能并没有在 DStream API 中提供,然而你可以简单的利用 transform 方法做到.

    import org.apache.spark.streaming.api.java.*;
    // RDD containing spam information
    JavaPairRDD<String, Double> spamInfoRDD = jssc.sparkContext().newAPIHadoopRDD(...);

    JavaPairDStream<String, Integer> cleanedDStream = wordCounts.transform(rdd -> {
    rdd.join(spamInfoRDD).filter(...); // join data stream with spam information to do data cleaning
    ...
    });

transform方法在每个批次都会进行调用, 因此可以根据不同时间进行相应的处理.

//待完善4

但需要注意的一点是:

虽然transform是  transformation算子, 但是 并不意味着其中的方法必然是在job分配完, 真实提交之后才执行.

原因也正是在于这个算子的灵活性相当高. 可以在其中嵌入任何RDD操作.

> 参考链接: [Spark Streaming 误用.transform(func)函数导致的问题解析](https://www.jianshu.com/p/69c64a920da0)

导致其问题的根本原因就在于 在 transform中执行的 action操作, 是会在 生成job的时候执行的.

#### Window

Spark Streaming 也支持 windowed computations(窗口计算),它允许你在数据的一个滑动窗口上应用 transformation(转换).下图说明了这个滑动窗口.

![](http://spark.apache.org/docs/latest/img/streaming-dstream-window.png)

如上图显示,窗口在源 DStream 上 slides(滑动),合并和操作落入窗内的源 RDDs,产生窗口化的 DStream 的 RDDs.在这个具体的例子中,程序在三个时间单元的数据上进行窗口操作,并且每两个时间单元滑动一次.这说明,任何一个窗口操作都需要指定两个参数.

window length(窗口长度) - 窗口的持续时间.

sliding interval(滑动间隔) - 执行窗口操作的间隔.

这两个参数必须是 source DStream 的 batch interval(批间隔)的倍数.

    // Reduce last 30 seconds of data, every 10 seconds
    JavaPairDStream<String, Integer> windowedWordCounts = pairs.reduceByKeyAndWindow((i1, i2) -> i1 + i2, Durations.seconds(30), Durations.seconds(10));

常用的 window操作如下:

Transformation(转换)| Meaning(含义)
-|-|-
window(windowLength, slideInterval)	| 返回一个新的 DStream,它是基于 source DStream 的窗口 batch 进行计算的.
countByWindow(windowLength, slideInterval) | 返回 stream(流)中滑动窗口元素的数
reduceByWindow(func, windowLength, slideInterval) | 返回一个新的单元素 stream(流),它通过在一个滑动间隔的 stream 中使用 func 来聚合以创建.该函数应该是 associative(关联的)且 commutative(可交换的),以便它可以并行计算
reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]) | 在一个 (K, V) pairs 的 DStream 上调用时,返回一个新的 (K, V) pairs 的 Stream,其中的每个 key 的 values 是在滑动窗口上的 batch 使用给定的函数 func 来聚合产生的.Note(注意): 默认情况下,该操作使用 Spark 的默认并行任务数量(local model 是 2,在 cluster mode 中的数量通过 spark.default.parallelism 来确定)来做 grouping.您可以通过一个可选的 numTasks 参数来设置一个不同的 tasks(任务)数量.
reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) | 上述 reduceByKeyAndWindow() 的更有效的一个版本,其中使用前一窗口的 reduce 值逐渐计算每个窗口的 reduce值.这是通过减少进入滑动窗口的新数据,以及 “inverse reducing(逆减)” 离开窗口的旧数据来完成的.一个例子是当窗口滑动时”添加” 和 “减” keys 的数量.然而,它仅适用于 “invertible reduce functions(可逆减少函数)”,即具有相应 “inverse reduce(反向减少)” 函数的 reduce 函数(作为参数 invFunc </ i>).像在 reduceByKeyAndWindow 中的那样,reduce 任务的数量可以通过可选参数进行配置.请注意,针对该操作的使用必须启用 checkpointing.
countByValueAndWindow(windowLength, slideInterval, [numTasks]) | 在一个 (K, V) pairs 的 DStream 上调用时,返回一个新的 (K, Long) pairs 的 DStream,其中每个 key 的 value 是它在一个滑动窗口之内的频次.像 code>reduceByKeyAndWindow</code> 中的那样,reduce 任务的数量可以通过可选参数进行配置.

如果可以, 并且一般都可以调用 reduceByKeyAndWindow 的第二个版本, 描述了那么多,其实旨在说明, 当采用滑动窗口的时候, 有两种对数据的处理方式, 其一是  每次都去统计最新的 窗口中的 所有数据. 其二则是, 在原有数据基础上做出一定更新,这就要求 对已经离开窗口的数据做 ‘减量’ 操作, 对新进入窗口的数据做 ‘增量’ 操作.也即需要提供的 参数:

func 对数据做reduce操作, 完成统计更新

invFunc 对数据做‘减量’ 操作 在原有基础上进行更新.

#### Join

join 与 Sql中的 Join极为类似, 在 SQL中 join操作,要以 join on “on”后面的参数为准, 而在 Spark中, 则要求 进行join 的两个 RDD 或 DStream 都必须是 PairRDD 或是 PairDStream. 

这样在进行join操作的时候, SQL中的 “on” 在Spark中就变成了, Tuple2中的第一个参数.

需要注意的是 join操作最终合并成一个流, 因此也会将多个分区的数据进行合并, 是一次窄依赖变换, 因此最终会形成新的分区.同时一般可以自行制定分区数, 如果不指定, 则使用Spark默认的分区数.

>至于Spark默认的分区数: 参考链接:[Spark RDD的默认分区数:(spark 2.1.0)](https://www.jianshu.com/p/4b7d07e754fa)

其核心就是:

sc.defaultParallelism = spark.default.parallelism
sc.defaultMinPartitions = min(spark.default.parallelism,2)

而RDD的分区数,则是 如果在重新分区时指定了分区数, 则采用分区数, 否则,就使用默认值.

另外, 分区的默认方式/规则 是 HashPartition

1. Join

    Join:Join类似于SQL的inner join操作,返回结果是前面和后面集合中配对成功的,过滤掉关联不上的.

2. leftOuterJoin
 
    leftOuterJoin类似于SQL中的左外关联left outer join,返回结果以前面的RDD为主,关联不上的记录为空.从返回类型上就可略知一二.

        JavaPairDStream[K, (V, Optional[W])]

    其第二个参数是 Optional, 可以接收空值.

3. rightOuterJoin
   
    rightOuterJoin类似于SQL中的有外关联right outer join,返回结果以参数也就是右边的RDD为主,关联不上的记录为空.

        JavaPairDStream[K, (Optional[V], W)]

4. fullOuterJoin

    fullOuterJoin相比前几种来说并不常见,是 左外 右外连接的 结合. 最终的结果 是两个 流的并集. 返回的数据类型是:

        JavaPairDStream[K, (Optional[V], Optional[W])]

对于Join还有一种不错的用法:

    JavaPairRDD<String, String> dataset = ...
    JavaPairDStream<String, String> windowedStream = stream.window(Durations.seconds(20));
    JavaPairDStream<String, String> joinedStream = windowedStream.transform(rdd -> rdd.join(dataset));

可以将RDD 与 流数据进行join操作, 进而完成流数据与 固定数据集的合并.

实际上,您也可以动态更改要加入的 dataset.提供给 transform 的函数是每个 batch interval(批次间隔)进行评估,因此可以将 dataset 引用指向当前的 dataset.

偶然间看到的案例:

>[Spark Streaming 流计算优化记录(2)-不同时间片数据流的Join](https://blog.csdn.net/butterluo/article/details/47083891)

主要解决的就是 来自 HDFS的数据 与 流数据合并.

但在这个案例中是不是有一种更合理的处理方式? 即每隔固定时间去 更新dataset, 在transform中 将流的RDD 与 HDFS的数据合并.

如果是不同的 batch interval 之间的数据可以合并吗?

目前来看并没有看到这种可能性, 对于 SparkContext, 同时只能够启动一个 SparkStreamingContext, 而在SparkStreamingContext启动时就需要指定 batch interval, 因此好像不太可能出现 多个 batchInterval.

倒不如说是 JavaPairRDD 与 JavaPairDStream 之间的合并.

#### DStreams 上的输出操作

输出操作 | 解释
-|-|-
print() | 在运行流应用程序的 driver 节点上的DStream中打印每批数据的前十个元素.这对于开发和调试很有用.
saveAsTextFiles(prefix, [suffix]) | 将此 DStream 的内容另存为文本文件.每个批处理间隔的文件名是根据 前缀 和 后缀:"prefix-TIMEIN_MS[.suffix]"_ 生成的.
saveAsObjectFiles(prefix, [suffix]) | 将此 DStream 的内容另存为序列化 Java 对象的 SequenceFiles.每个批处理间隔的文件名是根据 前缀 和 后缀:"prefix-TIMEIN_MS[.suffix]"_ 生成的.
saveAsHadoopFiles(prefix, [suffix]) | 将此 DStream 的内容另存为 Hadoop 文件.每个批处理间隔的文件名是根据 前缀 和 后缀:"prefix-TIMEIN_MS[.suffix]"_ 生成的.
foreachRDD(func) | 对从流中生成的每个 RDD 应用函数 func 的最通用的输出运算符.此功能应将每个 RDD 中的数据推送到外部系统,例如将 RDD 保存到文件,或将其通过网络写入数据库.请注意,函数 func 在运行流应用程序的 driver 进程中执行,通常会在其中具有 RDD 动作,这将强制流式传输 RDD 的计算.

<font color="orange"> 需要注意的是: foreachRDD 本身是运行在 Driver节点的, 而通常要对 RDD做相应的 action 操作. 而这部分操作则是在 各自的 work 上执行的. </font>

#### foreachRDD 设计模式的使用

错误操作:

    dstream.foreachRDD(rdd -> {
        Connection connection = createNewConnection(); // 在driver节点执行
        rdd.foreach(record -> {
            connection.send(record); // 在 worker节点执行.
        });
    });

而Connection往往又无法被序列化, 因此在 worker节点上依然拿不到连接.

    dstream.foreachRDD(rdd -> {
        rdd.foreach(record -> {
            Connection connection = createNewConnection();
            connection.send(record);
            connection.close();
        });
    });

因此往往需要采用如下这种方式:

    dstream.foreachRDD(rdd -> {
        rdd.foreachPartition(partitionOfRecords -> {
            // ConnectionPool is a static, lazily initialized pool of connections
            Connection connection = ConnectionPool.getConnection();
            while (partitionOfRecords.hasNext()) {
            connection.send(partitionOfRecords.next());
            }
            ConnectionPool.returnConnection(connection); // return to the pool for future reuse
        });
    });

如果仅仅是想单纯的处理数据, 但并不在这一就需要进行foreachRDD 用以完成流的执行计算呢?

伪代码如下:

    dstreamTemp = dstream.foreachRDD(rdd -> {
        rdd.foreachPartition(partitionOfRecords -> {
            Connection connection = ConnectionPool.getConnection();
            while (partitionOfRecords.hasNext()) {
                //进行查询数据, 填充 补全, 过滤 当前stream中的数据, 用以下一步继续处理.
            }
            ConnectionPool.returnConnection(connection); // return to the pool for future reuse
        });
    });

    dstreamTemp.nextAction;

但foreachRDD 是不会返回流的, 因此可以采用

    dstreamTemp = dstream.mapPairtition();
    dstreamTemp = dstream.mapToPairPartition();

另外关于 foreachRDD 需要注意的是:

DStreams 通过输出操作进行延迟执行,就像 RDD 由 RDD 操作懒惰地执行.具体来说,DStream 输出操作中的 RDD 动作强制处理接收到的数据.因此,如果您的应用程序没有任何输出操作,或者具有 dstream.foreachRDD() 等输出操作,而在其中没有任何 RDD 操作,则不会执行任何操作.系统将简单地接收数据并将其丢弃.

默认情况下,输出操作是 one-at-a-time 执行的, 且按照它们在应用程序中定义的顺序执行.

</font>